---
phase: 01-fix-ui-freeze
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ScreenSort/Services/OCRService.swift
autonomous: true

must_haves:
  truths:
    - "Vision OCR runs on background thread, not main thread"
    - "UI remains responsive during OCR processing"
  artifacts:
    - path: "ScreenSort/Services/OCRService.swift"
      provides: "Background-dispatched Vision OCR"
      contains: "DispatchQueue.global"
  key_links:
    - from: "OCRService.recognizeText(in:)"
      to: "VNImageRequestHandler.perform()"
      via: "DispatchQueue.global().async with continuation"
      pattern: "withCheckedThrowingContinuation.*DispatchQueue\\.global"
---

<objective>
Move Vision OCR processing off the main thread by wrapping the synchronous `handler.perform()` call in a background dispatch queue with async continuation.

Purpose: The Vision framework's `VNImageRequestHandler.perform()` is synchronous and blocks whatever thread it runs on. Currently it blocks the main thread, causing UI freeze. This plan fixes the root cause at the OCR service level.

Output: Modified OCRService.swift with background-dispatched Vision processing.
</objective>

<execution_context>
@/Users/mit/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mit/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-fix-ui-freeze/01-RESEARCH.md

# Source file to modify
@ScreenSort/Services/OCRService.swift
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wrap Vision perform() in background DispatchQueue</name>
  <files>ScreenSort/Services/OCRService.swift</files>
  <action>
Modify the `recognizeText(in:minimumConfidence:)` method (lines 24-92) to dispatch the synchronous `handler.perform([request])` call to a background queue using `withCheckedThrowingContinuation` pattern.

Specifically:
1. After creating the `VNImageRequestHandler`, wrap ALL the processing from `handler.perform()` through result mapping in a `withCheckedThrowingContinuation` block
2. Inside the continuation, dispatch to `DispatchQueue.global(qos: .userInitiated).async`
3. Inside the async block:
   - Call `handler.perform([request])` (this now runs on background thread)
   - Map the results to `TextObservation` array
   - Call `continuation.resume(returning: observations)` or `continuation.resume(throwing: error)` as appropriate
4. Handle all error cases properly (no text found, recognition failed, etc.)

The resulting structure should be:
```swift
func recognizeText(in image: UIImage, minimumConfidence: Float = 0.0) async throws -> [TextObservation] {
    guard let cgImage = image.cgImage else {
        throw OCRError.invalidImage
    }

    let request = VNRecognizeTextRequest()
    request.recognitionLevel = .accurate
    request.usesLanguageCorrection = true
    request.recognitionLanguages = ["en-US"]

    let orientation = cgImageOrientation(from: image.imageOrientation)
    let handler = VNImageRequestHandler(cgImage: cgImage, orientation: orientation, options: [:])

    return try await withCheckedThrowingContinuation { continuation in
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])

                guard let results = request.results, !results.isEmpty else {
                    continuation.resume(throwing: OCRError.noTextFound)
                    return
                }

                // Map results to TextObservation...
                // (existing mapping logic)

                continuation.resume(returning: sortedObservations)
            } catch {
                continuation.resume(throwing: OCRError.recognitionFailed(reason: error.localizedDescription))
            }
        }
    }
}
```

Important: Keep ALL existing functionality (confidence filtering, sorting, warning logging). Only change WHERE the code runs, not WHAT it does.
  </action>
  <verify>
Build the project with `xcodebuild -project ScreenSort.xcodeproj -scheme ScreenSort -destination 'platform=iOS Simulator,name=iPhone 16' build 2>&1 | tail -20` - should compile without errors.
  </verify>
  <done>
OCRService.swift compiles and `handler.perform([request])` is wrapped in `DispatchQueue.global(qos: .userInitiated).async` inside a `withCheckedThrowingContinuation` block.
  </done>
</task>

</tasks>

<verification>
1. Code compiles without errors
2. `DispatchQueue.global` is used to dispatch Vision processing
3. `withCheckedThrowingContinuation` bridges the callback-based dispatch to async/await
4. All error handling paths call `continuation.resume` exactly once
</verification>

<success_criteria>
- OCRService.swift compiles
- Vision `perform()` call is dispatched to background queue
- Continuation pattern correctly bridges async/await
- No functional changes to OCR behavior (same inputs produce same outputs)
</success_criteria>

<output>
After completion, create `.planning/phases/01-fix-ui-freeze/01-01-SUMMARY.md`
</output>
